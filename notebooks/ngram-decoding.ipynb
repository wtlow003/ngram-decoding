{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57cc9ac2-9ca4-4d26-b3f0-a16f3c9b9754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.33.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate) (0.24.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate) (0.4.4)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9810e81-8779-4978-a5a7-e07af768c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d445fb81-91e6-4ff8-b283-a1610a60bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate_tokens(\n",
    "    input_ids: torch.Tensor, n_grams: torch.Tensor, ngrams_size: int, K: int\n",
    "):\n",
    "    # unfold the tensor into windows of `pattern_len + following_elements_count`\n",
    "    window = input_ids.unfold(dimension=1, size=ngrams_size, step=1)\n",
    "    # compare each window with the pattern (only the parts corresponding to the pattern)\n",
    "    matching_window_indices = (window == n_grams).all(dim=2)\n",
    "    # extract the indices where there are matches\n",
    "    matching_indices = matching_window_indices.nonzero(as_tuple=True)[1]\n",
    "\n",
    "    # find candidates with the longest length\n",
    "    # based on: https://arxiv.org/pdf/2304.04487\n",
    "    # we choose the candidate with the longest length at random if there are multiple candidates\n",
    "    candidates = []\n",
    "    max_length = K\n",
    "    for idx in matching_indices:\n",
    "        start_idx = idx + ngrams_size\n",
    "        end_idx = start_idx + K\n",
    "        candidate = input_ids[0, start_idx : min(end_idx, input_ids.size(1))]\n",
    "        length = len(candidate)\n",
    "\n",
    "        if length == max_length:\n",
    "            candidates.append(candidate)\n",
    "        else:\n",
    "            # we do not consider prefix with no candidates\n",
    "            if length > max_length:\n",
    "                max_length = length\n",
    "                candidates = [candidate]\n",
    "\n",
    "    if candidates:\n",
    "        chosen_candidate = candidates[np.random.randint(len(candidates))]\n",
    "    else:\n",
    "        chosen_candidate = torch.tensor([], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "    return chosen_candidate.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc0fda5-cf45-44a9-89c1-ba99e35b155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decoding(\n",
    "    input_ids: torch.Tensor,\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n",
    "    n: int = 400,\n",
    "):\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    seq_len = input_ids.shape[1]\n",
    "    T = seq_len + n\n",
    "\n",
    "    while input_ids.shape[1] < T:\n",
    "        logits = model(input_ids).logits\n",
    "        next_token_id = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(dim=1)], dim=1)\n",
    "        yield next_token_id.item()\n",
    "        if next_token_id == eos_token_id:\n",
    "            break\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c440087-3d62-4ad1-bf53-d7decdad89ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ngram_decoding(\n",
    "    input_ids: torch.Tensor,\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n",
    "    ngrams_size: int,\n",
    "    K: int,\n",
    "    n: int,\n",
    "):\n",
    "    eos_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 0\n",
    "    eos_token_id_tensor = torch.tensor(\n",
    "        [eos_token_id], dtype=torch.long, device=input_ids.device\n",
    "    )\n",
    "    seq_len = input_ids.shape[1]\n",
    "    T = seq_len + n\n",
    "\n",
    "    while input_ids.shape[1] < T:\n",
    "        prefix = input_ids\n",
    "        cur_len = input_ids.shape[1]\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Step 1: Generate N-grams\n",
    "        # -----------------------------------------\n",
    "\n",
    "        n_grams = input_ids[0, -ngrams_size:]\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Step 2: Generate K candidates tokens using the N-grams\n",
    "        # -----------------------------------------\n",
    "\n",
    "        candidate_tokens = generate_candidate_tokens(input_ids, n_grams, ngrams_size, K)\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Step 3: Validate the candidates using the LLM\n",
    "        # -----------------------------------------\n",
    "\n",
    "        # based on: https://arxiv.org/pdf/2304.04487\n",
    "        # if we did not find any candidates tokens, we default to single-step decoding\n",
    "        if candidate_tokens.shape[1] == 0:\n",
    "            logits = model(input_ids).logits[:, -1, :]\n",
    "            next_token = logits.argmax(dim=-1)\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(dim=0)], dim=1)\n",
    "            yield (next_token.item(), False)\n",
    "            if next_token.item() == eos_token_id:\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        prefix = torch.cat([input_ids, candidate_tokens], dim=1)\n",
    "        # include the ngram_size + K + 1 in the logits\n",
    "        logits = model(prefix).logits[:, cur_len - 1 : cur_len + ngrams_size + K, :]\n",
    "\n",
    "        assert (\n",
    "            logits.shape[1] == candidate_tokens.shape[1] + 1\n",
    "        ), f\"Expected logits shape: {ngrams_size + K + 1}, got: {logits.shape[1]}\"\n",
    "\n",
    "        selected_tokens = logits.argmax(dim=-1)\n",
    "        # calculate the number of consecutive matching tokens between candidate_tokens and selected_tokens:\n",
    "        # 1. Compare candidate_tokens with selected_tokens\n",
    "        # 2. Invert the comparison result\n",
    "        # 3. Calculate cumulative sum of mismatches\n",
    "        # 4. Create a mask for positions before the first mismatch\n",
    "        # 5. Sum up the mask to get the count of consecutive matches\n",
    "        n_matches = (\n",
    "            (~(candidate_tokens == selected_tokens[:, :-1])).cumsum(dim=-1) < 1\n",
    "        ).sum()\n",
    "        n_matches = min(n_matches, T - cur_len - 1)\n",
    "\n",
    "        valid_tokens = selected_tokens[:, : n_matches + 1]\n",
    "        # print(\"selected from prompt: \", tokenizer.decode(valid_tokens[0]))\n",
    "        for token_id in valid_tokens[0]:\n",
    "            yield (token_id.item(), True)\n",
    "        input_ids = torch.cat([input_ids, valid_tokens], dim=1)\n",
    "\n",
    "        if input_ids.shape[1] >= T:  # Check if we've reached the desired length\n",
    "            break\n",
    "        # we fulfill the condition of ngrams_size + K\n",
    "        elif n_matches == ngrams_size + K:\n",
    "            # we can take the last token from the logits and append it to the input_ids\n",
    "            # we generated K+1 from the previous forward pass\n",
    "            next_token = selected_tokens[-1]\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            yield (next_token.item(), True)\n",
    "            if next_token == eos_token_id:\n",
    "                break\n",
    "\n",
    "        if (valid_tokens == eos_token_id_tensor.item()).any():\n",
    "            break\n",
    "\n",
    "    return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9f7d9a-6f7f-4948-a3ad-abfa77e74fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e733376-d315-4002-ab44-d50cbc0dc1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23623be06ea4d0b8dac486fe3cb5625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=DEVICE,\n",
    "        use_cache=False,\n",
    "    ).eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=DEVICE,\n",
    ")\n",
    "\n",
    "tokenizer.eos_token_id = 128009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a50f8e2-cbe7-4990-801c-74d17f7d04b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"\"\"\n",
    "def generate_candidate_tokens(\n",
    "    input_ids: torch.Tensor, n_grams: torch.Tensor, ngrams_size: int, K: int\n",
    "):\n",
    "    # unfold the tensor into windows of `pattern_len + following_elements_count`\n",
    "    window = input_ids.unfold(dimension=1, size=ngrams_size, step=1)\n",
    "    # compare each window with the pattern (only the parts corresponding to the pattern)\n",
    "    matching_window_indices = (window == n_grams).all(dim=2)\n",
    "    # extract the indices where there are matches\n",
    "    matching_indices = matching_window_indices.nonzero(as_tuple=True)[1]\n",
    "\n",
    "    # find candidates with the longest length\n",
    "    # based on: https://arxiv.org/pdf/2304.04487\n",
    "    # we choose the candidate with the longest length at random if there are multiple candidates\n",
    "    candidates = []\n",
    "    max_length = K\n",
    "    for idx in matching_indices:\n",
    "        start_idx = idx + ngrams_size\n",
    "        end_idx = start_idx + K\n",
    "        candidate = input_ids[0, start_idx : min(end_idx, input_ids.size(1))]\n",
    "        length = len(candidate)\n",
    "\n",
    "        if length == max_length:\n",
    "            candidates.append(candidate)\n",
    "        else:\n",
    "            # we do not consider prefix with no candidates\n",
    "            if length > max_length:\n",
    "                max_length = length\n",
    "                candidates = [candidate]\n",
    "\n",
    "    if candidates:\n",
    "        chosen_candidate = candidates[np.random.randint(len(candidates))]\n",
    "    else:\n",
    "        chosen_candidate = torch.tensor([], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "    return chosen_candidate.unsqueeze(dim=0)\n",
    "\"\"\"\n",
    "question = \"Can you the variable name 'candidates' to 'candidates_tokens'?\"\n",
    "prompt = \"<|start_header_id|>user<|end_header_id|>\\nCode:```python\\n{code_text}``` \\n\\n Question: {question} \\n\\n Modified code:\\n<|start_header_id|>assistant<|end_header_id|>\".format(\n",
    "    code_text=input_str, question=question\n",
    ")\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b7fde89-ea4e-47d2-aa2d-50f4937f06fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting warm-up run\n",
      "Warm-up complete.\n",
      "\n",
      "Ngram Decoding:\n",
      "\n",
      "\n",
      "Here is the modified code with the variable name\u001b[92m '\u001b[0m\u001b[92mcandidates\u001b[0m\u001b[92m'\u001b[0m\u001b[92m changed\u001b[0m to 'candidates\u001b[92m_tokens\u001b[0m\u001b[92m':\n",
      "\n",
      "\u001b[0m```python\n",
      "def generate_candidate\u001b[92m_tokens\u001b[0m\u001b[92m(\n",
      "\u001b[0m\u001b[92m   \u001b[0m\u001b[92m input\u001b[0m\u001b[92m_ids\u001b[0m\u001b[92m:\u001b[0m\u001b[92m torch\u001b[0m\u001b[92m.Tensor\u001b[0m\u001b[92m,\u001b[0m\u001b[92m n\u001b[0m\u001b[92m_\u001b[0m\u001b[92mgrams\u001b[0m\u001b[92m:\u001b[0m\u001b[92m torch\u001b[0m\u001b[92m.Tensor\u001b[0m\u001b[92m,\u001b[0m\u001b[92m n\u001b[0m\u001b[92mgrams\u001b[0m\u001b[92m_size\u001b[0m\u001b[92m:\u001b[0m\u001b[92m int\u001b[0m\u001b[92m,\u001b[0m\u001b[92m K\u001b[0m\u001b[92m:\u001b[0m\u001b[92m int\u001b[0m\u001b[92m\n",
      "\u001b[0m\u001b[92m):\n",
      "\u001b[0m\u001b[92m   \u001b[0m\u001b[92m #\u001b[0m\u001b[92m unfold\u001b[0m\u001b[92m the\u001b[0m\u001b[92m tensor\u001b[0m\u001b[92m into\u001b[0m\u001b[92m windows\u001b[0m\u001b[92m of\u001b[0m\u001b[92m `\u001b[0m\u001b[92mpattern\u001b[0m\u001b[92m_len\u001b[0m\u001b[92m +\u001b[0m\u001b[92m following\u001b[0m\u001b[92m_elements\u001b[0m\u001b[92m_count\u001b[0m\u001b[92m`\n",
      "\u001b[0m\u001b[92m   \u001b[0m\u001b[92m window\u001b[0m\u001b[92m =\u001b[0m\u001b[92m input\u001b[0m\u001b[92m_ids\u001b[0m\u001b[92m.un\u001b[0m\u001b[92mfold\u001b[0m\u001b[92m(d\u001b[0m\u001b[92mimension\u001b[0m\u001b[92m=\u001b[0m\u001b[92m1\u001b[0m\u001b[92m,\u001b[0m\u001b[92m size\u001b[0m\u001b[92m=n\u001b[0m\u001b[92mgrams\u001b[0m\u001b[92m_size\u001b[0m\u001b[92m,\u001b[0m\u001b[92m step\u001b[0m\u001b[92m=\u001b[0m\u001b[92m1\u001b[0m\u001b[92m)\n",
      "\u001b[0m\u001b[92m   \u001b[0m\u001b[92m #\u001b[0m\u001b[92m compare\u001b[0m\u001b[92m each\u001b[0m\u001b[92m window\u001b[0m\u001b[92m with\u001b[0m\u001b[92m the\u001b[0m\u001b[92m pattern\u001b[0m\u001b[92m (\u001b[0m\u001b[92monly\u001b[0m\u001b[92m the\u001b[0m\u001b[92m parts\u001b[0m\u001b[92m corresponding\u001b[0m\u001b[92m to\u001b[0m\u001b[92m the\u001b[0m\u001b[92m pattern\u001b[0m\u001b[92m)\n",
      "\u001b[0m\u001b[92m   \u001b[0m\u001b[92m matching\u001b[0m\u001b[92m_window\u001b[0m\u001b[92m_indices\u001b[0m\u001b[92m =\u001b[0m\u001b[92m (\u001b[0m\u001b[92mwindow\u001b[0m\u001b[92m ==\u001b[0m\u001b[92m n\u001b[0m\u001b[92m_\u001b[0m\u001b[92mgrams\u001b[0m\u001b[92m).\u001b[0m\u001b[92mall\u001b[0m\u001b[92m(dim\u001b[0m\u001b[92m=\u001b[0m\u001b[92m2\u001b[0m\u001b[92m)\n",
      "\u001b[0m\u001b[92m   \u001b[0m\u001b[92m #\u001b[0m\u001b[92m extract\u001b[0m\u001b[92m the\u001b[0m\u001b[92m indices\u001b[0m\u001b[92m where\u001b[0m\u001b[92m there\u001b[0m\u001b[92m are\u001b[0m\u001b[92m matches\u001b[0m\u001b[92m\n",
      "\u001b[0m\u001b[92m   \u001b[0m\u001b[92m matching\u001b[0m\u001b[92m_indices\u001b[0m\u001b[92m =\u001b[0m\u001b[92m matching\u001b[0m\u001b[92m_window\u001b[0m\u001b[92m_indices\u001b[0m\u001b[92m.non\u001b[0m\u001b[92mzero\u001b[0m\u001b[92m(as\u001b[0m\u001b[92m_tuple\u001b[0m\u001b[92m=True\u001b[0m\u001b[92m)[\u001b[0m\u001b[92m1\u001b[0m\u001b[92m]\n",
      "\n",
      "\u001b[0m\u001b[92m   \u001b[0m\u001b[92m #\u001b[0m\u001b[92m find\u001b[0m\u001b[92m candidates\u001b[0m\u001b[92m with\u001b[0m\u001b[92m the\u001b[0m\u001b[92m longest\u001b[0m\u001b[92m length\u001b[0m\u001b[92m\n",
      "\u001b[0m\u001b[92m   \u001b[0m\u001b[92m #\u001b[0m\u001b[92m based\u001b[0m\u001b[92m on\u001b[0m\u001b[92m:\u001b[0m\u001b[92m https\u001b[0m\u001b[92m://\u001b[0m\u001b[92mar\u001b[0m\u001b[92mxiv\u001b[0m\u001b[92m.org\u001b[0m\u001b[92m/pdf\u001b[0m\u001b[92m/\u001b[0m\u001b[92m230\u001b[0m\u001b[92m4\u001b[0m\u001b[92m.\u001b[0m\u001b[92m044\u001b[0m\u001b[92m87\u001b[0m\u001b[92m\n",
      "\u001b[0m\u001b[92m   \u001b[0m\u001b[92m #\u001b[0m\u001b[92m we\u001b[0m\u001b[92m choose\u001b[0m\u001b[92m the\u001b[0m\u001b[92m candidate\u001b[0m\u001b[92m with\u001b[0m\u001b[92m the\u001b[0m\u001b[92m longest\u001b[0m\u001b[92m length\u001b[0m\u001b[92m at\u001b[0m\u001b[92m random\u001b[0m\u001b[92m if\u001b[0m\u001b[92m there\u001b[0m\u001b[92m are\u001b[0m\u001b[92m multiple\u001b[0m\u001b[92m candidates\u001b[0m\u001b[92m\n",
      "\u001b[0m\u001b[92m   \u001b[0m\u001b[92m candidates\u001b[0m\u001b[92m_tokens\u001b[0m = []\n",
      "   \u001b[92m max\u001b[0m\u001b[92m_length\u001b[0m\u001b[92m =\u001b[0m\u001b[92m K\u001b[0m\u001b[92m\n",
      "\u001b[0m\u001b[92m   \u001b[0m\u001b[92m for\u001b[0m\u001b[92m idx\u001b[0m\u001b[92m in\u001b[0m\u001b[92m matching\u001b[0m\u001b[92m_indices\u001b[0m\u001b[92m:\n",
      "\u001b[0m\u001b[92m       \u001b[0m\u001b[92m start\u001b[0m\u001b[92m_idx\u001b[0m\u001b[92m =\u001b[0m\u001b[92m idx\u001b[0m\u001b[92m +\u001b[0m\u001b[92m n\u001b[0m\u001b[92mgrams\u001b[0m\u001b[92m_size\u001b[0m\u001b[92m\n",
      "\u001b[0m\u001b[92m       \u001b[0m\u001b[92m end\u001b[0m\u001b[92m_idx\u001b[0m\u001b[92m =\u001b[0m\u001b[92m start\u001b[0m\u001b[92m_idx\u001b[0m\u001b[92m +\u001b[0m\u001b[92m K\u001b[0m\u001b[92m\n",
      "\u001b[0m\u001b[92m       \u001b[0m\u001b[92m candidate\u001b[0m\u001b[92m =\u001b[0m\u001b[92m input\u001b[0m\u001b[92m_ids\u001b[0m\u001b[92m[\u001b[0m\u001b[92m0\u001b[0m\u001b[92m,\u001b[0m\u001b[92m start\u001b[0m\u001b[92m_idx\u001b[0m\u001b[92m :\u001b[0m\u001b[92m min\u001b[0m\u001b[92m(end\u001b[0m\u001b[92m_idx\u001b[0m\u001b[92m,\u001b[0m\u001b[92m input\u001b[0m\u001b[92m_ids\u001b[0m\u001b[92m.size\u001b[0m\u001b[92m(\u001b[0m\u001b[92m1\u001b[0m\u001b[92m))]\n",
      "\u001b[0m\u001b[92m       \u001b[0m\u001b[92m length\u001b[0m\u001b[92m =\u001b[0m\u001b[92m len\u001b[0m\u001b[92m(candidate\u001b[0m\u001b[92m)\n",
      "\n",
      "\u001b[0m\u001b[92m       \u001b[0m\u001b[92m if\u001b[0m\u001b[92m length\u001b[0m\u001b[92m ==\u001b[0m\u001b[92m max\u001b[0m\u001b[92m_length\u001b[0m\u001b[92m:\n",
      "\u001b[0m\u001b[92m           \u001b[0m\u001b[92m candidates\u001b[0m\u001b[92m_tokens\u001b[0m.append(candidate)\n",
      "\u001b[92m       \u001b[0m\u001b[92m else\u001b[0m\u001b[92m:\n",
      "\u001b[0m\u001b[92m           \u001b[0m\u001b[92m #\u001b[0m\u001b[92m we\u001b[0m\u001b[92m do\u001b[0m\u001b[92m not\u001b[0m\u001b[92m consider\u001b[0m\u001b[92m prefix\u001b[0m\u001b[92m with\u001b[0m\u001b[92m no\u001b[0m\u001b[92m candidates\u001b[0m\u001b[92m\n",
      "\u001b[0m\u001b[92m           \u001b[0m\u001b[92m if\u001b[0m\u001b[92m length\u001b[0m\u001b[92m >\u001b[0m\u001b[92m max\u001b[0m\u001b[92m_length\u001b[0m\u001b[92m:\n",
      "\u001b[0m\u001b[92m               \u001b[0m\u001b[92m max\u001b[0m\u001b[92m_length\u001b[0m\u001b[92m =\u001b[0m\u001b[92m length\u001b[0m\u001b[92m\n",
      "\u001b[0m\u001b[92m               \u001b[0m\u001b[92m candidates\u001b[0m\u001b[92m_tokens\u001b[0m =\u001b[92m [\u001b[0mcandidate\u001b[92m]\n",
      "\n",
      "\u001b[0m\u001b[92m   \u001b[0m\u001b[92m if\u001b[0m\u001b[92m candidates\u001b[0m\u001b[92m_tokens\u001b[0m:\n",
      "        chosen\u001b[92m_candidate\u001b[0m\u001b[92m =\u001b[0m\u001b[92m candidates\u001b[0m\u001b[92m_tokens\u001b[0m[np.random.randint\u001b[92m(len\u001b[0m\u001b[92m(c\u001b[0m\u001b[92mandidates\u001b[0m\u001b[92m_tokens\u001b[0m))]\n",
      "    else\u001b[92m:\n",
      "\u001b[0m\u001b[92m       \u001b[0m\u001b[92m chosen\u001b[0m\u001b[92m_candidate\u001b[0m\u001b[92m =\u001b[0m\u001b[92m torch\u001b[0m\u001b[92m.tensor\u001b[0m\u001b[92m([],\u001b[0m\u001b[92m dtype\u001b[0m\u001b[92m=torch\u001b[0m\u001b[92m.long\u001b[0m\u001b[92m,\u001b[0m\u001b[92m device\u001b[0m\u001b[92m=input\u001b[0m\u001b[92m_ids\u001b[0m\u001b[92m.device\u001b[0m\u001b[92m)\n",
      "\n",
      "\u001b[0m\u001b[92m   \u001b[0m\u001b[92m return\u001b[0m\u001b[92m chosen\u001b[0m\u001b[92m_candidate\u001b[0m\u001b[92m.unsqueeze\u001b[0m\u001b[92m(dim\u001b[0m\u001b[92m=\u001b[0m\u001b[92m0\u001b[0m\u001b[92m)\n",
      "\u001b[0m\u001b[92m``\u001b[0m`\n",
      "\n",
      "I changed the variable name\u001b[92m '\u001b[0m\u001b[92mcandidates\u001b[0m\u001b[92m'\u001b[0m\u001b[92m to\u001b[0m\u001b[92m '\u001b[0m\u001b[92mcandidates\u001b[0m\u001b[92m_tokens\u001b[0m\u001b[92m'\u001b[0m in the function `generate_candidate_tokens`.\n",
      "Time taken: 7.14641685411334 seconds, 52.473849154791075 tokens/s\n"
     ]
    }
   ],
   "source": [
    "# warm-up run\n",
    "print(\"Starting warm-up run\")\n",
    "ngram_decoding(input_ids, model, tokenizer, ngrams_size=3, K=10, n=50)\n",
    "print(\"Warm-up complete.\")\n",
    "\n",
    "# actual run\n",
    "print(\"\\nNgram Decoding:\")\n",
    "torch.cuda.synchronize()\n",
    "nd_start = time.perf_counter()\n",
    "nd_output_ids = []\n",
    "for token_id, speculated in ngram_decoding(\n",
    "    input_ids, model, tokenizer, ngrams_size=3, K=10, n=400\n",
    "):\n",
    "    nd_output_ids.append(token_id)\n",
    "    if speculated:\n",
    "        print(\n",
    "            f\"\\033[92m{tokenizer.decode(token_id)}\\033[0m\", end=\"\", flush=True\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            tokenizer.decode(token_id, skip_special_tokens=True),\n",
    "            end=\"\",\n",
    "            flush=True,\n",
    "        )\n",
    "torch.cuda.synchronize()\n",
    "nd_end = time.perf_counter()\n",
    "nd_time = nd_end - nd_start\n",
    "print(\n",
    "    f\"\\nTime taken: {nd_end - nd_start} seconds, {len(nd_output_ids) / nd_time} tokens/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1b2fe61-04d8-4835-99a0-561464e82f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting warm-up run\n",
      "Warm-up complete.\n",
      "\n",
      "Greedy Decoding:\n",
      "\n",
      "\n",
      "Here is the modified code with the variable name 'candidates' changed to 'candidates_tokens':\n",
      "\n",
      "```python\n",
      "def generate_candidate_tokens(\n",
      "    input_ids: torch.Tensor, n_grams: torch.Tensor, ngrams_size: int, K: int\n",
      "):\n",
      "    # unfold the tensor into windows of `pattern_len + following_elements_count`\n",
      "    window = input_ids.unfold(dimension=1, size=ngrams_size, step=1)\n",
      "    # compare each window with the pattern (only the parts corresponding to the pattern)\n",
      "    matching_window_indices = (window == n_grams).all(dim=2)\n",
      "    # extract the indices where there are matches\n",
      "    matching_indices = matching_window_indices.nonzero(as_tuple=True)[1]\n",
      "\n",
      "    # find candidates with the longest length\n",
      "    # based on: https://arxiv.org/pdf/2304.04487\n",
      "    # we choose the candidate with the longest length at random if there are multiple candidates\n",
      "    candidates_tokens = []\n",
      "    max_length = K\n",
      "    for idx in matching_indices:\n",
      "        start_idx = idx + ngrams_size\n",
      "        end_idx = start_idx + K\n",
      "        candidate = input_ids[0, start_idx : min(end_idx, input_ids.size(1))]\n",
      "        length = len(candidate)\n",
      "\n",
      "        if length == max_length:\n",
      "            candidates_tokens.append(candidate)\n",
      "        else:\n",
      "            # we do not consider prefix with no candidates\n",
      "            if length > max_length:\n",
      "                max_length = length\n",
      "                candidates_tokens = [candidate]\n",
      "\n",
      "    if candidates_tokens:\n",
      "        chosen_candidate = candidates_tokens[np.random.randint(len(candidates_tokens))]\n",
      "    else:\n",
      "        chosen_candidate = torch.tensor([], dtype=torch.long, device=input_ids.device)\n",
      "\n",
      "    return chosen_candidate.unsqueeze(dim=0)\n",
      "```\n",
      "\n",
      "I changed the variable name 'candidates' to 'candidates_tokens' in the function `generate_candidate_tokens`.\n",
      "Time taken: 24.267626140266657 seconds, 15.452685723461514 tokens/s\n"
     ]
    }
   ],
   "source": [
    "# warm-up run\n",
    "print(\"Starting warm-up run\")\n",
    "greedy_decoding(input_ids, model, tokenizer, n=50)\n",
    "print(\"Warm-up complete.\")\n",
    "\n",
    "print(\"\\nGreedy Decoding:\")\n",
    "torch.cuda.synchronize()\n",
    "gd_start = time.perf_counter()\n",
    "gd_output_ids = []\n",
    "for token_id in greedy_decoding(input_ids, model, tokenizer, n=400):\n",
    "    gd_output_ids.append(token_id)\n",
    "    print(\n",
    "        tokenizer.decode(token_id, skip_special_tokens=True), end=\"\", flush=True\n",
    "    )\n",
    "torch.cuda.synchronize()\n",
    "gd_end = time.perf_counter()\n",
    "gd_time = gd_end - gd_start\n",
    "print(\n",
    "    f\"\\nTime taken: {gd_end - gd_start} seconds, {len(gd_output_ids) / gd_time} tokens/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05cd79dd-54af-4b1a-a990-8e4cc90dfa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c7e22-c3da-4932-9635-31317ae1346f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
